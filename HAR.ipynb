{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MGsl81LFSVOr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "4lliAluRm23e",
        "colab_type": "code",
        "outputId": "48919894-6a11-4c8f-d431-328186467a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "path =  'x1_train.txt'\n",
        "x1 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x1.shape)\n",
        "path = 'y1_train.txt'\n",
        "x2 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x2.shape)\n",
        "path ='z1_train.txt'\n",
        "x3 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x3.shape)\n",
        "path =   'x2_train.txt'\n",
        "x4 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x4.shape)\n",
        "path = 'y2_train.txt'\n",
        "x5 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x5.shape)\n",
        "path = 'z2_train.txt'\n",
        "x6 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "print(x6.shape)\n",
        "path =  'x3_train.txt'\n",
        "x7 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "\n",
        "path =  'y3_train.txt'\n",
        "x8 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "\n",
        "path =  'z3_train.txt'\n",
        "x9 = pd.read_csv(path, header=None, sep='\\s+')\n",
        "\n",
        "\n",
        "\n",
        "print(x1.loc[1,1])\n",
        "######\n",
        "#reshape x\n",
        "x1=x1.values.tolist()\n",
        "x1=np.array(x1).reshape((7352,128,1))\n",
        "\n",
        "x2=x2.values.tolist()\n",
        "x2=np.array(x2).reshape((7352,128,1))\n",
        "\n",
        "x3=x3.values.tolist()\n",
        "x3=np.array(x3).reshape((7352,128,1))\n",
        "x4=x4.values.tolist()\n",
        "x4=np.array(x4).reshape((7352,128,1))\n",
        "\n",
        "x5=x5.values.tolist()\n",
        "x5=np.array(x5).reshape((7352,128,1))\n",
        "\n",
        "x6=x6.values.tolist()\n",
        "x6=np.array(x6).reshape((7352,128,1))\n",
        "x7=x7.values.tolist()\n",
        "x7=np.array(x7).reshape((7352,128,1))\n",
        "\n",
        "x8=x8.values.tolist()\n",
        "x8=np.array(x8).reshape((7352,128,1))\n",
        "\n",
        "x9=x9.values.tolist()\n",
        "x9=np.array(x9).reshape((7352,128,1))\n",
        "X_train=np.concatenate((x1,x2,x3,x4,x5,x6,x7,x8,x9),axis=2)\n",
        "print(X_train.shape)\n",
        "####\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7352, 128)\n",
            "(7352, 128)\n",
            "(7352, 128)\n",
            "(7352, 128)\n",
            "(7352, 128)\n",
            "(7352, 128)\n",
            "0.0045500770000000005\n",
            "(7352, 128, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yASQ8tgasTSA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_Z(z):\n",
        "    a=np.zeros((z.shape[0],30))\n",
        "    for j in range(0,z.shape[0]):\n",
        "      for i in range(30):\n",
        "        if z[j,0]==i+1:\n",
        "                a[j,i]=1\n",
        "    return a  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNuWZzgabaPP",
        "colab_type": "code",
        "outputId": "0c009354-fb53-43e5-a743-4b2ec2cdd5c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1235
        }
      },
      "cell_type": "code",
      "source": [
        "path = 'subject_train.txt'\n",
        "Z_train = pd.read_csv(path, header=None)\n",
        "print(Z_train)\n",
        "Z_train=convert_Z(np.array(Z_train))\n",
        "print(np.sum(Z_train,axis=0))\n",
        "\n",
        "Z_train=Z_train[:,np.sum(Z_train,axis=0)!=0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       0\n",
            "0      1\n",
            "1      1\n",
            "2      1\n",
            "3      1\n",
            "4      1\n",
            "5      1\n",
            "6      1\n",
            "7      1\n",
            "8      1\n",
            "9      1\n",
            "10     1\n",
            "11     1\n",
            "12     1\n",
            "13     1\n",
            "14     1\n",
            "15     1\n",
            "16     1\n",
            "17     1\n",
            "18     1\n",
            "19     1\n",
            "20     1\n",
            "21     1\n",
            "22     1\n",
            "23     1\n",
            "24     1\n",
            "25     1\n",
            "26     1\n",
            "27     1\n",
            "28     1\n",
            "29     1\n",
            "...   ..\n",
            "7322  30\n",
            "7323  30\n",
            "7324  30\n",
            "7325  30\n",
            "7326  30\n",
            "7327  30\n",
            "7328  30\n",
            "7329  30\n",
            "7330  30\n",
            "7331  30\n",
            "7332  30\n",
            "7333  30\n",
            "7334  30\n",
            "7335  30\n",
            "7336  30\n",
            "7337  30\n",
            "7338  30\n",
            "7339  30\n",
            "7340  30\n",
            "7341  30\n",
            "7342  30\n",
            "7343  30\n",
            "7344  30\n",
            "7345  30\n",
            "7346  30\n",
            "7347  30\n",
            "7348  30\n",
            "7349  30\n",
            "7350  30\n",
            "7351  30\n",
            "\n",
            "[7352 rows x 1 columns]\n",
            "[347.   0. 341.   0. 302. 325. 308. 281.   0.   0. 316.   0.   0. 323.\n",
            " 328. 366. 368.   0. 360.   0. 408. 321. 372.   0. 409. 392. 376. 382.\n",
            " 344. 383.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rhhuZ71bd183",
        "colab_type": "code",
        "outputId": "8a69182a-a573-4e26-8a28-2edf91bf7364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "cell_type": "code",
      "source": [
        "path =  'y_train.txt'\n",
        "Y_train = pd.read_csv(path, header=None)\n",
        "\n",
        "print(Y_train.describe())\n",
        "Y_train=Y_train.values.tolist()\n",
        "Y_train=np.array(Y_train)\n",
        "print(Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 0\n",
            "count  7352.000000\n",
            "mean      3.643362\n",
            "std       1.744802\n",
            "min       1.000000\n",
            "25%       2.000000\n",
            "50%       4.000000\n",
            "75%       5.000000\n",
            "max       6.000000\n",
            "(7352, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nMN1INA6fW5d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l=[]\n",
        "for i in range(Y_train.shape[0]):\n",
        "  if Y_train[i] ==4 or Y_train[i] ==5:\n",
        "    l.append(i)\n",
        "             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NyDV5WiphJW0",
        "colab_type": "code",
        "outputId": "cab0603e-0374-483d-93f3-f16dd8e2689c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X=X_train[np.array(l),:,:]\n",
        "Y=Y_train[np.array(l),]\n",
        "Z=Z_train[np.array(l),]\n",
        "Z.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2660, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "HWvsyx7libsD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_to_label(y):\n",
        "    a=np.zeros((y.shape[0],1))\n",
        "    for j in range(0,y.shape[0]):\n",
        "        if y[j,0]==4:\n",
        "                a[j,0]=1\n",
        "    return a  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67KfOBnFimts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "label=convert_to_label(Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAny-Ho6gZll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test,z_train, z_test = train_test_split(X, label,Z ,test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5SERghjF4Dz",
        "colab_type": "code",
        "outputId": "2d525c64-04b7-4a00-c96e-b0d1f71f92bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "np.sum(z_test,axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14., 25., 22., 21., 28., 18., 15., 26., 21., 33., 24., 37., 34.,\n",
              "       22., 29., 27., 31., 30., 34., 21., 20.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "4p4lw90HSiea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ]
    },
    {
      "metadata": {
        "id": "L1fpc_oyn7TH",
        "colab_type": "code",
        "outputId": "618472d2-a072-4c61-9cc0-1a09f5fcd507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7924
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "n_classes = 1\n",
        "n_channels = 9\n",
        "\n",
        "seq_len = 128          # Number   time sequence\n",
        "learning_rate = 0.0001\n",
        "epochs = 1200\n",
        "keep_prob_=0.5\n",
        "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
        "labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
        " # (batch, 128, 9) --> (batch, 64, 18)\n",
        "conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=16, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 64, 18) --> (batch, 32, 36)\n",
        "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 32, 36) --> (batch, 16, 72)\n",
        "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 16, 72) --> (batch, 8, 144)\n",
        "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
        "\n",
        "# Flatten and add dropout\n",
        "flat = tf.reshape(max_pool_4, (-1, 8*144))\n",
        "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
        "    \n",
        "# Predictions\n",
        "logits = tf.layers.dense(flat, n_classes)\n",
        "prediction=tf.nn.sigmoid(logits)\n",
        "# Cost function and optimizer\n",
        "cost=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_,logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        " # Calculate the correct predictions\n",
        "correct_prediction = tf.to_float(tf.greater(prediction, 0.5))\n",
        "accuracy = tf.reduce_mean(tf.to_float(tf.equal(labels_, correct_prediction)))\n",
        "ACC=[]\n",
        "for i in range(20):\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    #iteration = 1\n",
        "   \n",
        "    # Loop over epochs\n",
        "    for e in range(epochs):\n",
        "        \n",
        "        # Loop over batches\n",
        "        \n",
        "      loss, _ =sess.run([cost, optimizer], feed_dict={inputs_ : X_train, labels_: y_train})\n",
        "            \n",
        "            \n",
        "    \n",
        "            \n",
        "            # Print at each 5 iters\n",
        "      if (e % 50 == 0):\n",
        "        acc=sess.run(accuracy, feed_dict={inputs_ : X_test, labels_: y_test})\n",
        "        print(\"Epoch: {}/{}\".format(e, epochs),\n",
        "             \"Train loss: {:6f}\".format(loss),\n",
        "             \"TEST acc: {:.6f}\".format(acc))\n",
        "    ACC.append(acc)            \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0/1200 Train loss: 0.693210 TEST acc: 0.473684\n",
            "Epoch: 50/1200 Train loss: 0.605877 TEST acc: 0.744361\n",
            "Epoch: 100/1200 Train loss: 0.373594 TEST acc: 0.855263\n",
            "Epoch: 150/1200 Train loss: 0.298192 TEST acc: 0.857143\n",
            "Epoch: 200/1200 Train loss: 0.282210 TEST acc: 0.866541\n",
            "Epoch: 250/1200 Train loss: 0.275248 TEST acc: 0.874060\n",
            "Epoch: 300/1200 Train loss: 0.260892 TEST acc: 0.874060\n",
            "Epoch: 350/1200 Train loss: 0.259311 TEST acc: 0.862782\n",
            "Epoch: 400/1200 Train loss: 0.249737 TEST acc: 0.883459\n",
            "Epoch: 450/1200 Train loss: 0.240640 TEST acc: 0.877820\n",
            "Epoch: 500/1200 Train loss: 0.233910 TEST acc: 0.881579\n",
            "Epoch: 550/1200 Train loss: 0.224688 TEST acc: 0.875940\n",
            "Epoch: 600/1200 Train loss: 0.209468 TEST acc: 0.877820\n",
            "Epoch: 650/1200 Train loss: 0.200167 TEST acc: 0.875940\n",
            "Epoch: 700/1200 Train loss: 0.187563 TEST acc: 0.894737\n",
            "Epoch: 750/1200 Train loss: 0.166269 TEST acc: 0.902256\n",
            "Epoch: 800/1200 Train loss: 0.150553 TEST acc: 0.909774\n",
            "Epoch: 850/1200 Train loss: 0.132961 TEST acc: 0.932331\n",
            "Epoch: 900/1200 Train loss: 0.119659 TEST acc: 0.932331\n",
            "Epoch: 950/1200 Train loss: 0.106121 TEST acc: 0.934211\n",
            "Epoch: 1000/1200 Train loss: 0.099229 TEST acc: 0.936090\n",
            "Epoch: 1050/1200 Train loss: 0.091181 TEST acc: 0.958647\n",
            "Epoch: 1100/1200 Train loss: 0.081153 TEST acc: 0.953008\n",
            "Epoch: 1150/1200 Train loss: 0.073825 TEST acc: 0.954887\n",
            "Epoch: 0/1200 Train loss: 0.682541 TEST acc: 0.537594\n",
            "Epoch: 50/1200 Train loss: 0.578028 TEST acc: 0.791353\n",
            "Epoch: 100/1200 Train loss: 0.380681 TEST acc: 0.843985\n",
            "Epoch: 150/1200 Train loss: 0.291549 TEST acc: 0.864662\n",
            "Epoch: 200/1200 Train loss: 0.279459 TEST acc: 0.870301\n",
            "Epoch: 250/1200 Train loss: 0.272478 TEST acc: 0.860902\n",
            "Epoch: 300/1200 Train loss: 0.267948 TEST acc: 0.870301\n",
            "Epoch: 350/1200 Train loss: 0.260782 TEST acc: 0.862782\n",
            "Epoch: 400/1200 Train loss: 0.255792 TEST acc: 0.868421\n",
            "Epoch: 450/1200 Train loss: 0.249148 TEST acc: 0.872180\n",
            "Epoch: 500/1200 Train loss: 0.237826 TEST acc: 0.868421\n",
            "Epoch: 550/1200 Train loss: 0.227692 TEST acc: 0.872180\n",
            "Epoch: 600/1200 Train loss: 0.212361 TEST acc: 0.879699\n",
            "Epoch: 650/1200 Train loss: 0.200186 TEST acc: 0.881579\n",
            "Epoch: 700/1200 Train loss: 0.185591 TEST acc: 0.887218\n",
            "Epoch: 750/1200 Train loss: 0.165790 TEST acc: 0.900376\n",
            "Epoch: 800/1200 Train loss: 0.148441 TEST acc: 0.909774\n",
            "Epoch: 850/1200 Train loss: 0.134933 TEST acc: 0.915414\n",
            "Epoch: 900/1200 Train loss: 0.119144 TEST acc: 0.928571\n",
            "Epoch: 950/1200 Train loss: 0.105565 TEST acc: 0.945489\n",
            "Epoch: 1000/1200 Train loss: 0.097956 TEST acc: 0.954887\n",
            "Epoch: 1050/1200 Train loss: 0.085526 TEST acc: 0.958647\n",
            "Epoch: 1100/1200 Train loss: 0.076762 TEST acc: 0.960526\n",
            "Epoch: 1150/1200 Train loss: 0.068206 TEST acc: 0.962406\n",
            "Epoch: 0/1200 Train loss: 0.695816 TEST acc: 0.505639\n",
            "Epoch: 50/1200 Train loss: 0.634822 TEST acc: 0.770677\n",
            "Epoch: 100/1200 Train loss: 0.422877 TEST acc: 0.832707\n",
            "Epoch: 150/1200 Train loss: 0.298359 TEST acc: 0.855263\n",
            "Epoch: 200/1200 Train loss: 0.289116 TEST acc: 0.864662\n",
            "Epoch: 250/1200 Train loss: 0.275673 TEST acc: 0.874060\n",
            "Epoch: 300/1200 Train loss: 0.268734 TEST acc: 0.881579\n",
            "Epoch: 350/1200 Train loss: 0.254540 TEST acc: 0.875940\n",
            "Epoch: 400/1200 Train loss: 0.243091 TEST acc: 0.877820\n",
            "Epoch: 450/1200 Train loss: 0.230498 TEST acc: 0.877820\n",
            "Epoch: 500/1200 Train loss: 0.211502 TEST acc: 0.887218\n",
            "Epoch: 550/1200 Train loss: 0.194316 TEST acc: 0.883459\n",
            "Epoch: 600/1200 Train loss: 0.168368 TEST acc: 0.892857\n",
            "Epoch: 650/1200 Train loss: 0.149775 TEST acc: 0.913534\n",
            "Epoch: 700/1200 Train loss: 0.132152 TEST acc: 0.919173\n",
            "Epoch: 750/1200 Train loss: 0.122968 TEST acc: 0.924812\n",
            "Epoch: 800/1200 Train loss: 0.111614 TEST acc: 0.941729\n",
            "Epoch: 850/1200 Train loss: 0.100328 TEST acc: 0.949248\n",
            "Epoch: 900/1200 Train loss: 0.092049 TEST acc: 0.937970\n",
            "Epoch: 950/1200 Train loss: 0.083540 TEST acc: 0.949248\n",
            "Epoch: 1000/1200 Train loss: 0.078836 TEST acc: 0.956767\n",
            "Epoch: 1050/1200 Train loss: 0.070819 TEST acc: 0.954887\n",
            "Epoch: 1100/1200 Train loss: 0.066226 TEST acc: 0.958647\n",
            "Epoch: 1150/1200 Train loss: 0.058736 TEST acc: 0.960526\n",
            "Epoch: 0/1200 Train loss: 0.697187 TEST acc: 0.488722\n",
            "Epoch: 50/1200 Train loss: 0.627944 TEST acc: 0.723684\n",
            "Epoch: 100/1200 Train loss: 0.434909 TEST acc: 0.828947\n",
            "Epoch: 150/1200 Train loss: 0.305022 TEST acc: 0.857143\n",
            "Epoch: 200/1200 Train loss: 0.285643 TEST acc: 0.864662\n",
            "Epoch: 250/1200 Train loss: 0.276284 TEST acc: 0.864662\n",
            "Epoch: 300/1200 Train loss: 0.273724 TEST acc: 0.864662\n",
            "Epoch: 350/1200 Train loss: 0.261857 TEST acc: 0.866541\n",
            "Epoch: 400/1200 Train loss: 0.253759 TEST acc: 0.874060\n",
            "Epoch: 450/1200 Train loss: 0.242887 TEST acc: 0.875940\n",
            "Epoch: 500/1200 Train loss: 0.229128 TEST acc: 0.874060\n",
            "Epoch: 550/1200 Train loss: 0.211818 TEST acc: 0.877820\n",
            "Epoch: 600/1200 Train loss: 0.192680 TEST acc: 0.879699\n",
            "Epoch: 650/1200 Train loss: 0.168275 TEST acc: 0.881579\n",
            "Epoch: 700/1200 Train loss: 0.151482 TEST acc: 0.894737\n",
            "Epoch: 750/1200 Train loss: 0.137461 TEST acc: 0.900376\n",
            "Epoch: 800/1200 Train loss: 0.128094 TEST acc: 0.915414\n",
            "Epoch: 850/1200 Train loss: 0.118246 TEST acc: 0.932331\n",
            "Epoch: 900/1200 Train loss: 0.108245 TEST acc: 0.936090\n",
            "Epoch: 950/1200 Train loss: 0.096428 TEST acc: 0.941729\n",
            "Epoch: 1000/1200 Train loss: 0.086970 TEST acc: 0.953008\n",
            "Epoch: 1050/1200 Train loss: 0.081446 TEST acc: 0.953008\n",
            "Epoch: 1100/1200 Train loss: 0.073435 TEST acc: 0.956767\n",
            "Epoch: 1150/1200 Train loss: 0.066893 TEST acc: 0.968045\n",
            "Epoch: 0/1200 Train loss: 0.689897 TEST acc: 0.520677\n",
            "Epoch: 50/1200 Train loss: 0.611004 TEST acc: 0.746241\n",
            "Epoch: 100/1200 Train loss: 0.396838 TEST acc: 0.842105\n",
            "Epoch: 150/1200 Train loss: 0.297760 TEST acc: 0.866541\n",
            "Epoch: 200/1200 Train loss: 0.281307 TEST acc: 0.864662\n",
            "Epoch: 250/1200 Train loss: 0.273816 TEST acc: 0.872180\n",
            "Epoch: 300/1200 Train loss: 0.266334 TEST acc: 0.870301\n",
            "Epoch: 350/1200 Train loss: 0.258865 TEST acc: 0.875940\n",
            "Epoch: 400/1200 Train loss: 0.249318 TEST acc: 0.872180\n",
            "Epoch: 450/1200 Train loss: 0.237206 TEST acc: 0.875940\n",
            "Epoch: 500/1200 Train loss: 0.219100 TEST acc: 0.889098\n",
            "Epoch: 550/1200 Train loss: 0.205215 TEST acc: 0.885338\n",
            "Epoch: 600/1200 Train loss: 0.188794 TEST acc: 0.877820\n",
            "Epoch: 650/1200 Train loss: 0.177380 TEST acc: 0.892857\n",
            "Epoch: 700/1200 Train loss: 0.160929 TEST acc: 0.900376\n",
            "Epoch: 750/1200 Train loss: 0.144310 TEST acc: 0.904135\n",
            "Epoch: 800/1200 Train loss: 0.129563 TEST acc: 0.915414\n",
            "Epoch: 850/1200 Train loss: 0.113179 TEST acc: 0.934211\n",
            "Epoch: 900/1200 Train loss: 0.097523 TEST acc: 0.941729\n",
            "Epoch: 950/1200 Train loss: 0.084372 TEST acc: 0.941729\n",
            "Epoch: 1000/1200 Train loss: 0.075001 TEST acc: 0.960526\n",
            "Epoch: 1050/1200 Train loss: 0.065126 TEST acc: 0.956767\n",
            "Epoch: 1100/1200 Train loss: 0.056810 TEST acc: 0.962406\n",
            "Epoch: 1150/1200 Train loss: 0.049035 TEST acc: 0.969925\n",
            "Epoch: 0/1200 Train loss: 0.699490 TEST acc: 0.475564\n",
            "Epoch: 50/1200 Train loss: 0.633716 TEST acc: 0.780075\n",
            "Epoch: 100/1200 Train loss: 0.424795 TEST acc: 0.840226\n",
            "Epoch: 150/1200 Train loss: 0.302891 TEST acc: 0.851504\n",
            "Epoch: 200/1200 Train loss: 0.284457 TEST acc: 0.868421\n",
            "Epoch: 250/1200 Train loss: 0.268713 TEST acc: 0.879699\n",
            "Epoch: 300/1200 Train loss: 0.255961 TEST acc: 0.870301\n",
            "Epoch: 350/1200 Train loss: 0.243746 TEST acc: 0.870301\n",
            "Epoch: 400/1200 Train loss: 0.229425 TEST acc: 0.875940\n",
            "Epoch: 450/1200 Train loss: 0.213825 TEST acc: 0.881579\n",
            "Epoch: 500/1200 Train loss: 0.196124 TEST acc: 0.890977\n",
            "Epoch: 550/1200 Train loss: 0.182483 TEST acc: 0.892857\n",
            "Epoch: 600/1200 Train loss: 0.163228 TEST acc: 0.904135\n",
            "Epoch: 650/1200 Train loss: 0.147559 TEST acc: 0.922932\n",
            "Epoch: 700/1200 Train loss: 0.133991 TEST acc: 0.930451\n",
            "Epoch: 750/1200 Train loss: 0.116522 TEST acc: 0.949248\n",
            "Epoch: 800/1200 Train loss: 0.100357 TEST acc: 0.945489\n",
            "Epoch: 850/1200 Train loss: 0.087340 TEST acc: 0.964286\n",
            "Epoch: 900/1200 Train loss: 0.079313 TEST acc: 0.958647\n",
            "Epoch: 950/1200 Train loss: 0.070028 TEST acc: 0.956767\n",
            "Epoch: 1000/1200 Train loss: 0.062207 TEST acc: 0.958647\n",
            "Epoch: 1050/1200 Train loss: 0.057696 TEST acc: 0.973684\n",
            "Epoch: 1100/1200 Train loss: 0.052153 TEST acc: 0.969925\n",
            "Epoch: 1150/1200 Train loss: 0.043578 TEST acc: 0.975564\n",
            "Epoch: 0/1200 Train loss: 0.693709 TEST acc: 0.543233\n",
            "Epoch: 50/1200 Train loss: 0.583042 TEST acc: 0.812030\n",
            "Epoch: 100/1200 Train loss: 0.372706 TEST acc: 0.832707\n",
            "Epoch: 150/1200 Train loss: 0.295520 TEST acc: 0.864662\n",
            "Epoch: 200/1200 Train loss: 0.287035 TEST acc: 0.874060\n",
            "Epoch: 250/1200 Train loss: 0.276989 TEST acc: 0.866541\n",
            "Epoch: 300/1200 Train loss: 0.271766 TEST acc: 0.864662\n",
            "Epoch: 350/1200 Train loss: 0.261307 TEST acc: 0.874060\n",
            "Epoch: 400/1200 Train loss: 0.250583 TEST acc: 0.866541\n",
            "Epoch: 450/1200 Train loss: 0.234845 TEST acc: 0.864662\n",
            "Epoch: 500/1200 Train loss: 0.219661 TEST acc: 0.879699\n",
            "Epoch: 550/1200 Train loss: 0.204681 TEST acc: 0.874060\n",
            "Epoch: 600/1200 Train loss: 0.188787 TEST acc: 0.874060\n",
            "Epoch: 650/1200 Train loss: 0.169410 TEST acc: 0.887218\n",
            "Epoch: 700/1200 Train loss: 0.148808 TEST acc: 0.906015\n",
            "Epoch: 750/1200 Train loss: 0.129038 TEST acc: 0.921053\n",
            "Epoch: 800/1200 Train loss: 0.116561 TEST acc: 0.924812\n",
            "Epoch: 850/1200 Train loss: 0.105433 TEST acc: 0.941729\n",
            "Epoch: 900/1200 Train loss: 0.095803 TEST acc: 0.949248\n",
            "Epoch: 950/1200 Train loss: 0.087905 TEST acc: 0.949248\n",
            "Epoch: 1000/1200 Train loss: 0.080016 TEST acc: 0.951128\n",
            "Epoch: 1050/1200 Train loss: 0.075965 TEST acc: 0.966165\n",
            "Epoch: 1100/1200 Train loss: 0.068643 TEST acc: 0.966165\n",
            "Epoch: 1150/1200 Train loss: 0.062845 TEST acc: 0.966165\n",
            "Epoch: 0/1200 Train loss: 0.704278 TEST acc: 0.466165\n",
            "Epoch: 50/1200 Train loss: 0.612523 TEST acc: 0.808271\n",
            "Epoch: 100/1200 Train loss: 0.381939 TEST acc: 0.834586\n",
            "Epoch: 150/1200 Train loss: 0.294212 TEST acc: 0.866541\n",
            "Epoch: 200/1200 Train loss: 0.277516 TEST acc: 0.868421\n",
            "Epoch: 250/1200 Train loss: 0.274283 TEST acc: 0.872180\n",
            "Epoch: 300/1200 Train loss: 0.265732 TEST acc: 0.885338\n",
            "Epoch: 350/1200 Train loss: 0.258626 TEST acc: 0.866541\n",
            "Epoch: 400/1200 Train loss: 0.248281 TEST acc: 0.870301\n",
            "Epoch: 450/1200 Train loss: 0.234640 TEST acc: 0.874060\n",
            "Epoch: 500/1200 Train loss: 0.221324 TEST acc: 0.874060\n",
            "Epoch: 550/1200 Train loss: 0.209541 TEST acc: 0.887218\n",
            "Epoch: 600/1200 Train loss: 0.194839 TEST acc: 0.890977\n",
            "Epoch: 650/1200 Train loss: 0.180395 TEST acc: 0.904135\n",
            "Epoch: 700/1200 Train loss: 0.162677 TEST acc: 0.913534\n",
            "Epoch: 750/1200 Train loss: 0.145998 TEST acc: 0.921053\n",
            "Epoch: 800/1200 Train loss: 0.126612 TEST acc: 0.939850\n",
            "Epoch: 850/1200 Train loss: 0.113122 TEST acc: 0.937970\n",
            "Epoch: 900/1200 Train loss: 0.100824 TEST acc: 0.947368\n",
            "Epoch: 950/1200 Train loss: 0.093287 TEST acc: 0.953008\n",
            "Epoch: 1000/1200 Train loss: 0.082198 TEST acc: 0.958647\n",
            "Epoch: 1050/1200 Train loss: 0.075119 TEST acc: 0.962406\n",
            "Epoch: 1100/1200 Train loss: 0.067256 TEST acc: 0.958647\n",
            "Epoch: 1150/1200 Train loss: 0.061115 TEST acc: 0.964286\n",
            "Epoch: 0/1200 Train loss: 0.698425 TEST acc: 0.424812\n",
            "Epoch: 50/1200 Train loss: 0.642863 TEST acc: 0.708647\n",
            "Epoch: 100/1200 Train loss: 0.456974 TEST acc: 0.838346\n",
            "Epoch: 150/1200 Train loss: 0.306245 TEST acc: 0.855263\n",
            "Epoch: 200/1200 Train loss: 0.283366 TEST acc: 0.866541\n",
            "Epoch: 250/1200 Train loss: 0.272261 TEST acc: 0.855263\n",
            "Epoch: 300/1200 Train loss: 0.268277 TEST acc: 0.853383\n",
            "Epoch: 350/1200 Train loss: 0.259590 TEST acc: 0.860902\n",
            "Epoch: 400/1200 Train loss: 0.254899 TEST acc: 0.864662\n",
            "Epoch: 450/1200 Train loss: 0.248665 TEST acc: 0.874060\n",
            "Epoch: 500/1200 Train loss: 0.236164 TEST acc: 0.875940\n",
            "Epoch: 550/1200 Train loss: 0.226504 TEST acc: 0.872180\n",
            "Epoch: 600/1200 Train loss: 0.215438 TEST acc: 0.868421\n",
            "Epoch: 650/1200 Train loss: 0.201548 TEST acc: 0.881579\n",
            "Epoch: 700/1200 Train loss: 0.181341 TEST acc: 0.898496\n",
            "Epoch: 750/1200 Train loss: 0.163038 TEST acc: 0.921053\n",
            "Epoch: 800/1200 Train loss: 0.141503 TEST acc: 0.936090\n",
            "Epoch: 850/1200 Train loss: 0.125619 TEST acc: 0.936090\n",
            "Epoch: 900/1200 Train loss: 0.107293 TEST acc: 0.937970\n",
            "Epoch: 950/1200 Train loss: 0.095038 TEST acc: 0.947368\n",
            "Epoch: 1000/1200 Train loss: 0.085872 TEST acc: 0.945489\n",
            "Epoch: 1050/1200 Train loss: 0.074373 TEST acc: 0.945489\n",
            "Epoch: 1100/1200 Train loss: 0.068133 TEST acc: 0.956767\n",
            "Epoch: 1150/1200 Train loss: 0.060282 TEST acc: 0.962406\n",
            "Epoch: 0/1200 Train loss: 0.692786 TEST acc: 0.520677\n",
            "Epoch: 50/1200 Train loss: 0.625982 TEST acc: 0.778196\n",
            "Epoch: 100/1200 Train loss: 0.396990 TEST acc: 0.840226\n",
            "Epoch: 150/1200 Train loss: 0.295103 TEST acc: 0.859023\n",
            "Epoch: 200/1200 Train loss: 0.282372 TEST acc: 0.860902\n",
            "Epoch: 250/1200 Train loss: 0.271861 TEST acc: 0.864662\n",
            "Epoch: 300/1200 Train loss: 0.264417 TEST acc: 0.881579\n",
            "Epoch: 350/1200 Train loss: 0.255763 TEST acc: 0.870301\n",
            "Epoch: 400/1200 Train loss: 0.247502 TEST acc: 0.881579\n",
            "Epoch: 450/1200 Train loss: 0.231455 TEST acc: 0.879699\n",
            "Epoch: 500/1200 Train loss: 0.215909 TEST acc: 0.877820\n",
            "Epoch: 550/1200 Train loss: 0.194031 TEST acc: 0.892857\n",
            "Epoch: 600/1200 Train loss: 0.171398 TEST acc: 0.911654\n",
            "Epoch: 650/1200 Train loss: 0.147804 TEST acc: 0.921053\n",
            "Epoch: 700/1200 Train loss: 0.130549 TEST acc: 0.924812\n",
            "Epoch: 750/1200 Train loss: 0.111730 TEST acc: 0.932331\n",
            "Epoch: 800/1200 Train loss: 0.093347 TEST acc: 0.953008\n",
            "Epoch: 850/1200 Train loss: 0.081706 TEST acc: 0.949248\n",
            "Epoch: 900/1200 Train loss: 0.071293 TEST acc: 0.964286\n",
            "Epoch: 950/1200 Train loss: 0.059794 TEST acc: 0.958647\n",
            "Epoch: 1000/1200 Train loss: 0.049362 TEST acc: 0.969925\n",
            "Epoch: 1050/1200 Train loss: 0.047438 TEST acc: 0.975564\n",
            "Epoch: 1100/1200 Train loss: 0.039795 TEST acc: 0.981203\n",
            "Epoch: 1150/1200 Train loss: 0.035882 TEST acc: 0.971804\n",
            "Epoch: 0/1200 Train loss: 0.697455 TEST acc: 0.492481\n",
            "Epoch: 50/1200 Train loss: 0.635840 TEST acc: 0.768797\n",
            "Epoch: 100/1200 Train loss: 0.439055 TEST acc: 0.838346\n",
            "Epoch: 150/1200 Train loss: 0.285617 TEST acc: 0.862782\n",
            "Epoch: 200/1200 Train loss: 0.278567 TEST acc: 0.862782\n",
            "Epoch: 250/1200 Train loss: 0.273910 TEST acc: 0.853383\n",
            "Epoch: 300/1200 Train loss: 0.264874 TEST acc: 0.860902\n",
            "Epoch: 350/1200 Train loss: 0.259932 TEST acc: 0.872180\n",
            "Epoch: 400/1200 Train loss: 0.252226 TEST acc: 0.872180\n",
            "Epoch: 450/1200 Train loss: 0.243131 TEST acc: 0.881579\n",
            "Epoch: 500/1200 Train loss: 0.229826 TEST acc: 0.885338\n",
            "Epoch: 550/1200 Train loss: 0.215275 TEST acc: 0.887218\n",
            "Epoch: 600/1200 Train loss: 0.198340 TEST acc: 0.898496\n",
            "Epoch: 650/1200 Train loss: 0.177160 TEST acc: 0.907895\n",
            "Epoch: 700/1200 Train loss: 0.150203 TEST acc: 0.919173\n",
            "Epoch: 750/1200 Train loss: 0.135407 TEST acc: 0.934211\n",
            "Epoch: 800/1200 Train loss: 0.120746 TEST acc: 0.936090\n",
            "Epoch: 850/1200 Train loss: 0.110113 TEST acc: 0.943609\n",
            "Epoch: 900/1200 Train loss: 0.098395 TEST acc: 0.941729\n",
            "Epoch: 950/1200 Train loss: 0.088195 TEST acc: 0.951128\n",
            "Epoch: 1000/1200 Train loss: 0.077072 TEST acc: 0.949248\n",
            "Epoch: 1050/1200 Train loss: 0.067733 TEST acc: 0.951128\n",
            "Epoch: 1100/1200 Train loss: 0.059728 TEST acc: 0.964286\n",
            "Epoch: 1150/1200 Train loss: 0.052987 TEST acc: 0.956767\n",
            "Epoch: 0/1200 Train loss: 0.693633 TEST acc: 0.524436\n",
            "Epoch: 50/1200 Train loss: 0.631620 TEST acc: 0.808271\n",
            "Epoch: 100/1200 Train loss: 0.410735 TEST acc: 0.827068\n",
            "Epoch: 150/1200 Train loss: 0.294344 TEST acc: 0.874060\n",
            "Epoch: 200/1200 Train loss: 0.279395 TEST acc: 0.862782\n",
            "Epoch: 250/1200 Train loss: 0.271479 TEST acc: 0.872180\n",
            "Epoch: 300/1200 Train loss: 0.262488 TEST acc: 0.866541\n",
            "Epoch: 350/1200 Train loss: 0.251771 TEST acc: 0.870301\n",
            "Epoch: 400/1200 Train loss: 0.244447 TEST acc: 0.877820\n",
            "Epoch: 450/1200 Train loss: 0.232871 TEST acc: 0.870301\n",
            "Epoch: 500/1200 Train loss: 0.215479 TEST acc: 0.879699\n",
            "Epoch: 550/1200 Train loss: 0.198994 TEST acc: 0.881579\n",
            "Epoch: 600/1200 Train loss: 0.170998 TEST acc: 0.898496\n",
            "Epoch: 650/1200 Train loss: 0.149605 TEST acc: 0.904135\n",
            "Epoch: 700/1200 Train loss: 0.134361 TEST acc: 0.913534\n",
            "Epoch: 750/1200 Train loss: 0.121007 TEST acc: 0.930451\n",
            "Epoch: 800/1200 Train loss: 0.109128 TEST acc: 0.941729\n",
            "Epoch: 850/1200 Train loss: 0.095778 TEST acc: 0.943609\n",
            "Epoch: 900/1200 Train loss: 0.088046 TEST acc: 0.947368\n",
            "Epoch: 950/1200 Train loss: 0.080553 TEST acc: 0.953008\n",
            "Epoch: 1000/1200 Train loss: 0.071762 TEST acc: 0.964286\n",
            "Epoch: 1050/1200 Train loss: 0.061963 TEST acc: 0.953008\n",
            "Epoch: 1100/1200 Train loss: 0.056187 TEST acc: 0.968045\n",
            "Epoch: 1150/1200 Train loss: 0.049724 TEST acc: 0.966165\n",
            "Epoch: 0/1200 Train loss: 0.685377 TEST acc: 0.537594\n",
            "Epoch: 50/1200 Train loss: 0.577290 TEST acc: 0.772556\n",
            "Epoch: 100/1200 Train loss: 0.362553 TEST acc: 0.838346\n",
            "Epoch: 150/1200 Train loss: 0.294403 TEST acc: 0.868421\n",
            "Epoch: 200/1200 Train loss: 0.283042 TEST acc: 0.872180\n",
            "Epoch: 250/1200 Train loss: 0.274800 TEST acc: 0.874060\n",
            "Epoch: 300/1200 Train loss: 0.266882 TEST acc: 0.866541\n",
            "Epoch: 350/1200 Train loss: 0.257125 TEST acc: 0.874060\n",
            "Epoch: 400/1200 Train loss: 0.255242 TEST acc: 0.872180\n",
            "Epoch: 450/1200 Train loss: 0.245147 TEST acc: 0.879699\n",
            "Epoch: 500/1200 Train loss: 0.231372 TEST acc: 0.875940\n",
            "Epoch: 550/1200 Train loss: 0.220916 TEST acc: 0.883459\n",
            "Epoch: 600/1200 Train loss: 0.206400 TEST acc: 0.877820\n",
            "Epoch: 650/1200 Train loss: 0.196371 TEST acc: 0.889098\n",
            "Epoch: 700/1200 Train loss: 0.179078 TEST acc: 0.890977\n",
            "Epoch: 750/1200 Train loss: 0.169949 TEST acc: 0.898496\n",
            "Epoch: 800/1200 Train loss: 0.155093 TEST acc: 0.898496\n",
            "Epoch: 850/1200 Train loss: 0.142182 TEST acc: 0.924812\n",
            "Epoch: 900/1200 Train loss: 0.126287 TEST acc: 0.924812\n",
            "Epoch: 950/1200 Train loss: 0.112508 TEST acc: 0.941729\n",
            "Epoch: 1000/1200 Train loss: 0.101448 TEST acc: 0.951128\n",
            "Epoch: 1050/1200 Train loss: 0.090023 TEST acc: 0.953008\n",
            "Epoch: 1100/1200 Train loss: 0.079155 TEST acc: 0.960526\n",
            "Epoch: 1150/1200 Train loss: 0.070080 TEST acc: 0.960526\n",
            "Epoch: 0/1200 Train loss: 0.689647 TEST acc: 0.530075\n",
            "Epoch: 50/1200 Train loss: 0.619163 TEST acc: 0.791353\n",
            "Epoch: 100/1200 Train loss: 0.432589 TEST acc: 0.838346\n",
            "Epoch: 150/1200 Train loss: 0.318772 TEST acc: 0.851504\n",
            "Epoch: 200/1200 Train loss: 0.287682 TEST acc: 0.860902\n",
            "Epoch: 250/1200 Train loss: 0.271691 TEST acc: 0.862782\n",
            "Epoch: 300/1200 Train loss: 0.258220 TEST acc: 0.864662\n",
            "Epoch: 350/1200 Train loss: 0.246550 TEST acc: 0.866541\n",
            "Epoch: 400/1200 Train loss: 0.232865 TEST acc: 0.870301\n",
            "Epoch: 450/1200 Train loss: 0.220815 TEST acc: 0.872180\n",
            "Epoch: 500/1200 Train loss: 0.209510 TEST acc: 0.883459\n",
            "Epoch: 550/1200 Train loss: 0.190699 TEST acc: 0.883459\n",
            "Epoch: 600/1200 Train loss: 0.170267 TEST acc: 0.909774\n",
            "Epoch: 650/1200 Train loss: 0.154092 TEST acc: 0.913534\n",
            "Epoch: 700/1200 Train loss: 0.133924 TEST acc: 0.930451\n",
            "Epoch: 750/1200 Train loss: 0.119478 TEST acc: 0.941729\n",
            "Epoch: 800/1200 Train loss: 0.106682 TEST acc: 0.947368\n",
            "Epoch: 850/1200 Train loss: 0.096765 TEST acc: 0.951128\n",
            "Epoch: 900/1200 Train loss: 0.087275 TEST acc: 0.964286\n",
            "Epoch: 950/1200 Train loss: 0.079700 TEST acc: 0.964286\n",
            "Epoch: 1000/1200 Train loss: 0.070100 TEST acc: 0.962406\n",
            "Epoch: 1050/1200 Train loss: 0.060863 TEST acc: 0.969925\n",
            "Epoch: 1100/1200 Train loss: 0.055518 TEST acc: 0.973684\n",
            "Epoch: 1150/1200 Train loss: 0.046141 TEST acc: 0.973684\n",
            "Epoch: 0/1200 Train loss: 0.690272 TEST acc: 0.528196\n",
            "Epoch: 50/1200 Train loss: 0.618381 TEST acc: 0.748120\n",
            "Epoch: 100/1200 Train loss: 0.427976 TEST acc: 0.834586\n",
            "Epoch: 150/1200 Train loss: 0.315643 TEST acc: 0.853383\n",
            "Epoch: 200/1200 Train loss: 0.292438 TEST acc: 0.866541\n",
            "Epoch: 250/1200 Train loss: 0.281856 TEST acc: 0.859023\n",
            "Epoch: 300/1200 Train loss: 0.274025 TEST acc: 0.868421\n",
            "Epoch: 350/1200 Train loss: 0.264947 TEST acc: 0.872180\n",
            "Epoch: 400/1200 Train loss: 0.247792 TEST acc: 0.874060\n",
            "Epoch: 450/1200 Train loss: 0.237969 TEST acc: 0.875940\n",
            "Epoch: 500/1200 Train loss: 0.221709 TEST acc: 0.874060\n",
            "Epoch: 550/1200 Train loss: 0.211308 TEST acc: 0.889098\n",
            "Epoch: 600/1200 Train loss: 0.200355 TEST acc: 0.898496\n",
            "Epoch: 650/1200 Train loss: 0.189886 TEST acc: 0.900376\n",
            "Epoch: 700/1200 Train loss: 0.179813 TEST acc: 0.917293\n",
            "Epoch: 750/1200 Train loss: 0.171880 TEST acc: 0.906015\n",
            "Epoch: 800/1200 Train loss: 0.157738 TEST acc: 0.913534\n",
            "Epoch: 850/1200 Train loss: 0.151926 TEST acc: 0.924812\n",
            "Epoch: 900/1200 Train loss: 0.142435 TEST acc: 0.915414\n",
            "Epoch: 950/1200 Train loss: 0.129833 TEST acc: 0.928571\n",
            "Epoch: 1000/1200 Train loss: 0.116647 TEST acc: 0.930451\n",
            "Epoch: 1050/1200 Train loss: 0.106489 TEST acc: 0.941729\n",
            "Epoch: 1100/1200 Train loss: 0.091842 TEST acc: 0.937970\n",
            "Epoch: 1150/1200 Train loss: 0.087655 TEST acc: 0.937970\n",
            "Epoch: 0/1200 Train loss: 0.697972 TEST acc: 0.481203\n",
            "Epoch: 50/1200 Train loss: 0.595842 TEST acc: 0.813910\n",
            "Epoch: 100/1200 Train loss: 0.387706 TEST acc: 0.832707\n",
            "Epoch: 150/1200 Train loss: 0.296955 TEST acc: 0.862782\n",
            "Epoch: 200/1200 Train loss: 0.283227 TEST acc: 0.864662\n",
            "Epoch: 250/1200 Train loss: 0.277195 TEST acc: 0.870301\n",
            "Epoch: 300/1200 Train loss: 0.265049 TEST acc: 0.872180\n",
            "Epoch: 350/1200 Train loss: 0.255720 TEST acc: 0.870301\n",
            "Epoch: 400/1200 Train loss: 0.247620 TEST acc: 0.879699\n",
            "Epoch: 450/1200 Train loss: 0.237690 TEST acc: 0.874060\n",
            "Epoch: 500/1200 Train loss: 0.227376 TEST acc: 0.875940\n",
            "Epoch: 550/1200 Train loss: 0.216579 TEST acc: 0.872180\n",
            "Epoch: 600/1200 Train loss: 0.200325 TEST acc: 0.875940\n",
            "Epoch: 650/1200 Train loss: 0.185678 TEST acc: 0.885338\n",
            "Epoch: 700/1200 Train loss: 0.166101 TEST acc: 0.896617\n",
            "Epoch: 750/1200 Train loss: 0.155671 TEST acc: 0.915414\n",
            "Epoch: 800/1200 Train loss: 0.143066 TEST acc: 0.917293\n",
            "Epoch: 850/1200 Train loss: 0.132537 TEST acc: 0.930451\n",
            "Epoch: 900/1200 Train loss: 0.121769 TEST acc: 0.926692\n",
            "Epoch: 950/1200 Train loss: 0.114442 TEST acc: 0.936090\n",
            "Epoch: 1000/1200 Train loss: 0.103250 TEST acc: 0.943609\n",
            "Epoch: 1050/1200 Train loss: 0.099556 TEST acc: 0.945489\n",
            "Epoch: 1100/1200 Train loss: 0.088341 TEST acc: 0.953008\n",
            "Epoch: 1150/1200 Train loss: 0.080922 TEST acc: 0.949248\n",
            "Epoch: 0/1200 Train loss: 0.698634 TEST acc: 0.451128\n",
            "Epoch: 50/1200 Train loss: 0.621033 TEST acc: 0.815789\n",
            "Epoch: 100/1200 Train loss: 0.414883 TEST acc: 0.834586\n",
            "Epoch: 150/1200 Train loss: 0.306402 TEST acc: 0.855263\n",
            "Epoch: 200/1200 Train loss: 0.280969 TEST acc: 0.874060\n",
            "Epoch: 250/1200 Train loss: 0.275151 TEST acc: 0.872180\n",
            "Epoch: 300/1200 Train loss: 0.269142 TEST acc: 0.879699\n",
            "Epoch: 350/1200 Train loss: 0.262452 TEST acc: 0.881579\n",
            "Epoch: 400/1200 Train loss: 0.253049 TEST acc: 0.889098\n",
            "Epoch: 450/1200 Train loss: 0.237369 TEST acc: 0.875940\n",
            "Epoch: 500/1200 Train loss: 0.221020 TEST acc: 0.881579\n",
            "Epoch: 550/1200 Train loss: 0.203132 TEST acc: 0.877820\n",
            "Epoch: 600/1200 Train loss: 0.179397 TEST acc: 0.883459\n",
            "Epoch: 650/1200 Train loss: 0.158841 TEST acc: 0.896617\n",
            "Epoch: 700/1200 Train loss: 0.142845 TEST acc: 0.909774\n",
            "Epoch: 750/1200 Train loss: 0.126617 TEST acc: 0.926692\n",
            "Epoch: 800/1200 Train loss: 0.115638 TEST acc: 0.934211\n",
            "Epoch: 850/1200 Train loss: 0.103631 TEST acc: 0.947368\n",
            "Epoch: 900/1200 Train loss: 0.091439 TEST acc: 0.953008\n",
            "Epoch: 950/1200 Train loss: 0.084130 TEST acc: 0.956767\n",
            "Epoch: 1000/1200 Train loss: 0.076169 TEST acc: 0.953008\n",
            "Epoch: 1050/1200 Train loss: 0.066694 TEST acc: 0.956767\n",
            "Epoch: 1100/1200 Train loss: 0.061053 TEST acc: 0.966165\n",
            "Epoch: 1150/1200 Train loss: 0.055023 TEST acc: 0.969925\n",
            "Epoch: 0/1200 Train loss: 0.691580 TEST acc: 0.528196\n",
            "Epoch: 50/1200 Train loss: 0.633200 TEST acc: 0.734962\n",
            "Epoch: 100/1200 Train loss: 0.433370 TEST acc: 0.836466\n",
            "Epoch: 150/1200 Train loss: 0.299751 TEST acc: 0.864662\n",
            "Epoch: 200/1200 Train loss: 0.283912 TEST acc: 0.870301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-615476ba08b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Loop over batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "v47VQaQAU-3j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ACC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CM3dprhFSsgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "nepePDL6JUH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNN COMBINED WITH GLMM"
      ]
    },
    {
      "metadata": {
        "id": "aWmtlYXrud30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n_classes = 1\n",
        "n_channels = 9\n",
        "\n",
        "seq_len = 128          # Number   time sequence\n",
        "learning_rate = 0.0001\n",
        "epochs = 150\n",
        "keep_prob_=0.5\n",
        "inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
        "Y_ = tf.placeholder(tf.float32, [None, 1], name = 'y')\n",
        " # (batch, 128, 9) --> (batch, 64, 18)\n",
        "conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=16, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 64, 18) --> (batch, 32, 36)\n",
        "conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=36, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 32, 36) --> (batch, 16, 72)\n",
        "conv3 = tf.layers.conv1d(inputs=max_pool_2, filters=72, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same')\n",
        "    \n",
        "    # (batch, 16, 72) --> (batch, 8, 144)\n",
        "conv4 = tf.layers.conv1d(inputs=max_pool_3, filters=144, kernel_size=2, strides=1, \n",
        "                             padding='same', activation = tf.nn.relu)\n",
        "max_pool_4 = tf.layers.max_pooling1d(inputs=conv4, pool_size=2, strides=2, padding='same')\n",
        "#print(max_pool_3.shape[-2])\n",
        "# Flatten and add dropout\n",
        "flat = tf.reshape(max_pool_4, (-1, 8*144))\n",
        "flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
        "    \n",
        "\n",
        "logits = tf.layers.dense(flat, n_classes)\n",
        "\n",
        "# Cost function and optimizer\n",
        "cost=cost=tf.losses.mean_squared_error(Y_,logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rxKyWo-TaHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Taingning data"
      ]
    },
    {
      "metadata": {
        "id": "xwphOL070t-V",
        "colab_type": "code",
        "outputId": "a0a3dd89-2847-48bb-a14a-f668d608d168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X=np.ones((y_train.shape[0],1))\n",
        "Y=y_train.reshape(y_train.shape[0],)\n",
        "Z=z_train\n",
        "X.shape,Y.shape,Z.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2128, 1), (2128,), (2128, 21))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "metadata": {
        "id": "LyH6BTfiTe-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing data"
      ]
    },
    {
      "metadata": {
        "id": "I1q99y6x1jE-",
        "colab_type": "code",
        "outputId": "b7af7423-ffd3-4d76-8d23-4050e1b73876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "Ytest=y_test.reshape(y_test.shape[0],)\n",
        "Xtest=np.ones((y_test.shape[0],1)).astype(float)\n",
        "Ztest=z_test\n",
        "#X_test #ppg signal\n",
        "print(Ytest.shape,Xtest.shape,Ztest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(532,) (532, 1) (532, 21)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZogHZ8bovOVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lamda(eita):\n",
        "    s=1/(1+np.exp(-eita))\n",
        "    lamda=(1/(2*eita))*(s-0.5)\n",
        "    return lamda\n",
        "\n",
        "\n",
        "def Estep(beta_old,sigma_old,Y,X,Z,epsilon):#Input old parameters and X,Y,Z\n",
        "    N=Y.shape[0]\n",
        "    #L=X.shape[1]\n",
        "    K=Z.shape[1]\n",
        "    Sigma=(1/sigma_old)*(np.ones(K))\n",
        "    \n",
        "    for i in range(N):\n",
        "        Sigma=Sigma+2*lamda(np.sqrt(epsilon[i]))*Z[i,]#(np.outer(Z[i,],Z[i,]).diagonal())\n",
        "    \n",
        "    Sigma1=1/(Sigma)\n",
        "    mu=np.zeros(K)\n",
        "    for i in range(N):\n",
        "        mu=mu+Z[i,]-2*Y[i]*Z[i,]+4*lamda(np.sqrt(epsilon[i]))*(np.dot(X[i,],beta_old))*Z[i,]\n",
        "    p=-0.5*Sigma1*mu\n",
        "    list1=[p,Sigma1]  #OUTPUT POSTERIOR mean and covariance matrix\n",
        "    return list1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rn00RTjpTVby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Initialization\n",
        "beta_old=np.random.random(X.shape[1])\n",
        "sigma_old=1\n",
        "gamma_old=1\n",
        "b_old=np.append(beta_old,gamma_old)\n",
        "N=X.shape[0]\n",
        "epsilon=np.zeros(N)\n",
        "\n",
        "for i in range(N):\n",
        "  epsilon[i]=sigma_old*np.dot(Z[i,],Z[i,])+(np.dot(X[i,],beta_old))**2\n",
        "\n",
        "p,Sigma=Estep(beta_old,sigma_old,Y,X,Z,epsilon)#initialize E step\n",
        "epsilon_new=epsilon#updatew epsilon\n",
        "for i in range(N):\n",
        "    epsilon_new[i]=np.dot(Sigma,Z[i,])+(np.dot(p,Z[i,]))**2+2*(np.dot(X[i,],beta_old))*(np.dot(Z[i,],p))+(np.dot(X[i,],beta_old))**2\n",
        "g=-1000000000#lower bound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKvit1vE14eF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y(beta_old,gamma_old,Y,X,Z,epsilon):\n",
        "    N=X.shape[0]\n",
        "    y_nn=np.zeros(N)\n",
        "    for i in range(N):\n",
        "        y_nn[i]=0.5/(lamda(np.sqrt(epsilon[i]))*gamma_old)*(Y[i]-0.5-2*lamda(np.sqrt(epsilon[i]))*(np.dot(X[i,],beta_old)+np.dot(Z[i,],p)))\n",
        "    return y_nn   \n",
        "        \n",
        "def bMstep(beta_old,sigma_old,Y,X,Z,epsilon,p,Sigma):\n",
        "    epsilon_new=epsilon\n",
        "    N=X.shape[0]\n",
        "    L=X.shape[1]\n",
        "    K=Z.shape[1]  \n",
        "    S=np.zeros((L,L))\n",
        "    M=np.zeros(L)\n",
        "    for i in range(N):\n",
        "        S=S+2*lamda(np.sqrt(epsilon_new[i]))*np.outer(X[i,],X[i,])\n",
        "        M=M+Y[i]*X[i,]-2*lamda(np.sqrt(epsilon_new[i]))*(np.dot(Z[i,],p))*X[i,]-0.5*X[i,]\n",
        "    beta_new=np.matmul(np.linalg.inv(S),M)\n",
        "    sigma_new=(np.dot(p,p)+np.sum(Sigma))/K\n",
        "    list3=[beta_new,sigma_new]\n",
        "    return list3        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cyDAXBJB1873",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prediction(x,beta,p,Sigma,z):#  Ep_i\n",
        "    mu=np.dot(z,p)\n",
        "    sigma=np.dot(Sigma,z)# posterior mean and sigma of random effects\n",
        "    def w(u):#pi\n",
        "        b=1/(1+np.exp(-np.dot(x,beta)-u))\n",
        "        return b\n",
        "    W=np.zeros(2000)\n",
        "    for i in range(2000):\n",
        "        u=np.random.normal(mu, sigma, 1)# Monte carlo method\n",
        "        W[i]=w(u)\n",
        "    return np.mean(W)\n",
        "def accuracy(X,Y,Z,beta_old,p,Sigma):\n",
        "  N=X.shape[0]\n",
        "  acc=0\n",
        "  Y_P=np.zeros(N)\n",
        "  for i in range(N):\n",
        "    if prediction(X[i,],beta_old,p,Sigma,Z[i,])>0.5:\n",
        "      Y_P[i]=1\n",
        "    if Y_P[i]==Y[i]:\n",
        "      acc=acc+1\n",
        "      \n",
        "  return acc/N  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IOhLg-UE2AwO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ACC2=[]\n",
        "for i in range(20):\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    \n",
        "    N=Y.shape[0]\n",
        "    L1=X.shape[1]\n",
        "    K=Z.shape[1] \n",
        "    # Loop over epochs\n",
        "    for e in range(epochs):\n",
        "          \n",
        "    \n",
        "   \n",
        "    #update cost and logits\n",
        "        epsilon=epsilon_new\n",
        "        Y_new=y(beta_old,gamma_old,Y,X,Z,epsilon)#  X , beta_old not x_com. b_old\n",
        "        #cost=tf.squre(Y_-logits)\n",
        "        \n",
        "        for u in range(100):\n",
        "          sess.run(optimizer, feed_dict={inputs_ :  X_train, Y_: Y_new.reshape(Y_new.shape[0],1)})#feed Y_:Y_neW\n",
        "       \n",
        "        logit,costs=sess.run([logits,cost], feed_dict={inputs_ :  X_train, Y_: Y_new.reshape(Y_new.shape[0],1)})\n",
        "        print(costs)\n",
        "        \n",
        "        b_old=np.append(beta_old,gamma_old)\n",
        "        X_com=np.concatenate((X,logit),axis=1)\n",
        "        b_old,sigma_old=bMstep(b_old,sigma_old,Y,X_com,Z,epsilon,p,Sigma)\n",
        "        beta_old=b_old[0:L1]\n",
        "        gamma_old=b_old[-1]\n",
        "        g_old=g\n",
        "        g=-0.5*K*np.log(sigma_old)+0.5*np.sum(np.log(Sigma))#lower bound\n",
        "        for i in range(N):\n",
        "            g=g+Y[i]*(np.dot(Z[i,],p)+np.dot(X_com[i,],b_old))-lamda(np.sqrt(epsilon[i]))*(np.dot(Sigma,Z[i,])+(np.dot(p,Z[i,]))**2+2*(np.dot(X_com[i,],b_old))*(np.dot(Z[i,],p))+(np.dot(X_com[i,],b_old))**2-epsilon[i])-0.5*(np.dot(Z[i,],p))-0.5*(np.dot(X_com[i,],b_old))-0.5*np.sqrt(epsilon[i])+np.log(1/(1+np.exp(-np.sqrt(epsilon[i]))))\n",
        "    #if gg_\n",
        "        if abs(g-g_old)<0.05:\n",
        "          break\n",
        "        #TEST ACCURACY\n",
        "        #Xtest =featuretest+1,Ytest=labeltest\n",
        "        if (e%50==49):\n",
        "          logitstest=sess.run(logits, feed_dict={inputs_ :  X_test})\n",
        "          X_comtest=np.concatenate((Xtest,logitstest),axis=1)\n",
        "          \n",
        "          acc=accuracy(X_com,Y,Z,b_old,p,Sigma)\n",
        "          print(acc,sigma_old)\n",
        "        #E step\n",
        "          p,Sigma=Estep(b_old,sigma_old,Y,X_com,Z,epsilon)#initialize\n",
        "         #updateepsilon\n",
        "          epsilon_new=epsilon\n",
        "        for i in range(N):\n",
        "            epsilon_new[i]=np.dot(Sigma,Z[i,])+(np.dot(p,Z[i,]))**2+2*(np.dot(X_com[i,],b_old))*(np.dot(Z[i,],p))+(np.dot(X_com[i,],b_old))**2\n",
        "        \n",
        "        print(g)\n",
        "    ACC2.append(acc)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_zK5PcClz7sP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " ACC2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}